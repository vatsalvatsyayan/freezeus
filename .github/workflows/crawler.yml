name: Run job crawler

on:
  schedule:
    # Three times daily: 08:00, 16:00, 00:00 UTC
    # (Approximately 3am, 11am, 7pm EST / 12am, 8am, 4pm PST)
    - cron: "0 8,16,0 * * *"
  # Allow manual runs from the Actions tab
  workflow_dispatch: {}

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 100  # Maximum runtime: 100 minutes

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python 3.13.5
        uses: actions/setup-python@v5
        with:
          python-version: "3.13.5"
          cache: 'pip'  # Cache pip dependencies for faster runs

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright browsers with system deps
        run: |
          python -m playwright install --with-deps chromium

      - name: Create configs/.env from GitHub Secrets
        run: |
          mkdir -p configs
          cat <<EOF > configs/.env
          # === Gemini API Configuration ===
          GEMINI_API_KEY=${{ secrets.GEMINI_API_KEY }}
          GEMINI_MODEL=${{ secrets.GEMINI_MODEL || 'models/gemini-1.5-pro-latest' }}
          LLM_MAX_HTML_CHARS=250000
          LLM_MAX_RETRIES=2
          LLM_RETRY_BASE_SLEEP=1.6
          LLM_VERBOSE=1
          LLM_OVERWRITE=0

          # === Supabase Database Configuration ===
          SUPABASE_ENABLED=1
          SUPABASE_URL=${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY=${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SUPABASE_JOBS_TABLE=jobs

          # === Crawler Configuration ===
          MAX_RETRIES=3
          NAV_TIMEOUT_MS=45000
          PER_DOMAIN_DELAY_MIN=8
          PER_DOMAIN_DELAY_MAX=15

          # === Logging Configuration ===
          LOG_LEVEL=INFO
          LOG_DIR=logs

          # === Error Logging Configuration ===
          ERROR_LOG_TABLE=error_logs
          ERROR_LOG_FALLBACK_DIR=logs/errors
          EOF

      - name: Show configs/.env (redacted)
        run: |
          echo "configs/.env created with the following keys:"
          sed 's/=.*/=[REDACTED]/' configs/.env

      - name: Verify configs/urls.txt exists
        run: |
          if [ ! -f configs/urls.txt ]; then
            echo "ERROR: configs/urls.txt not found"
            exit 1
          fi
          echo "Found $(wc -l < configs/urls.txt) URLs to crawl"

      - name: Run crawler (headless)
        env:
          LLM_MAX_HTML_CHARS: 500000
        run: |
          python -m src.crawler.multi_capture \
            --urls configs/urls.txt \
            --headless \
            --with-llm \
            --loadmore-max 15

      - name: Upload crawl artifacts (optional)
        if: always()  # Upload even if crawler fails
        uses: actions/upload-artifact@v4
        with:
          name: crawl-results-${{ github.run_number }}
          path: |
            out/*/manifest.json
            out/*/llm/*.jobs.json
          retention-days: 7
          if-no-files-found: ignore  # Don't fail if no files found

      - name: Report success
        if: success()
        run: |
          echo "✅ Crawl completed successfully"
          echo "Check Supabase for new job listings"

      - name: Report failure
        if: failure()
        run: |
          echo "❌ Crawl failed - check logs above"
          exit 1
