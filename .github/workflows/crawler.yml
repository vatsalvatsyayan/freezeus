name: Run job crawler

on:
  schedule:
    # Twice daily at 8 AM and 3 PM Los Angeles time (America/Los_Angeles)
    # LA is UTC-8 (PST) or UTC-7 (PDT during daylight saving)
    # 8 AM LA = 15:00 UTC (PDT) or 16:00 UTC (PST)
    # 3 PM LA = 22:00 UTC (PDT) or 23:00 UTC (PST)
    # Using PDT times (March-November): 15:00 and 22:00 UTC
    - cron: "0 15,22 * * *"
  # Allow manual runs from the Actions tab
  workflow_dispatch: {}

jobs:
  # First job: count URLs and generate chunk matrix
  setup:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Calculate chunks (5 URLs per chunk)
        id: set-matrix
        run: |
          URL_COUNT=$(grep -c '[^[:space:]]' configs/urls.txt)
          URLS_PER_CHUNK=5
          CHUNK_COUNT=$(( (URL_COUNT + URLS_PER_CHUNK - 1) / URLS_PER_CHUNK ))

          echo "Total URLs: $URL_COUNT"
          echo "URLs per chunk: $URLS_PER_CHUNK"
          echo "Chunks needed: $CHUNK_COUNT"

          # Generate matrix JSON: {"chunk": [0, 1, 2, ...]}
          CHUNKS=$(seq 0 $((CHUNK_COUNT - 1)) | jq -cs '{"chunk": .}')
          echo "matrix=$CHUNKS" >> $GITHUB_OUTPUT
          echo "Matrix: $CHUNKS"

  # Second job: crawl URLs in parallel chunks
  crawl:
    needs: setup
    runs-on: ubuntu-latest
    timeout-minutes: 30  # ~5 URLs should complete in ~20 minutes
    strategy:
      fail-fast: false  # Continue other jobs if one fails
      matrix: ${{ fromJson(needs.setup.outputs.matrix) }}

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python 3.13.5
        uses: actions/setup-python@v5
        with:
          python-version: "3.13.5"
          cache: 'pip'  # Cache pip dependencies for faster runs

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright browsers with system deps
        run: |
          python -m playwright install --with-deps chromium

      - name: Create configs/.env from GitHub Secrets
        run: |
          mkdir -p configs
          cat <<EOF > configs/.env
          # === Gemini API Configuration ===
          GEMINI_API_KEY=${{ secrets.GEMINI_API_KEY }}
          GEMINI_MODEL=${{ secrets.GEMINI_MODEL || 'models/gemini-1.5-pro-latest' }}
          LLM_MAX_HTML_CHARS=250000
          LLM_MAX_RETRIES=2
          LLM_RETRY_BASE_SLEEP=1.6
          LLM_VERBOSE=1
          LLM_OVERWRITE=0

          # === Supabase Database Configuration ===
          SUPABASE_ENABLED=1
          SUPABASE_URL=${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY=${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SUPABASE_JOBS_TABLE=jobs

          # === Crawler Configuration ===
          MAX_RETRIES=3
          NAV_TIMEOUT_MS=45000
          PER_DOMAIN_DELAY_MIN=8
          PER_DOMAIN_DELAY_MAX=15

          # === Logging Configuration ===
          LOG_LEVEL=INFO
          LOG_DIR=logs

          # === Error Logging Configuration ===
          ERROR_LOG_TABLE=error_logs
          ERROR_LOG_FALLBACK_DIR=logs/errors
          EOF

      - name: Show configs/.env (redacted)
        run: |
          echo "configs/.env created with the following keys:"
          sed 's/=.*/=[REDACTED]/' configs/.env

      - name: Verify configs/urls.txt exists
        run: |
          if [ ! -f configs/urls.txt ]; then
            echo "ERROR: configs/urls.txt not found"
            exit 1
          fi
          echo "Found $(grep -c '[^[:space:]]' configs/urls.txt) URLs total"

      - name: Create chunk-specific URL file
        run: |
          # Extract non-empty lines and split into chunks of 5
          grep '[^[:space:]]' configs/urls.txt > /tmp/all_urls.txt
          TOTAL=$(wc -l < /tmp/all_urls.txt)
          URLS_PER_CHUNK=5
          START=$(( ${{ matrix.chunk }} * URLS_PER_CHUNK + 1 ))
          END=$(( START + URLS_PER_CHUNK - 1 ))

          # Extract this chunk's URLs
          sed -n "${START},${END}p" /tmp/all_urls.txt > /tmp/chunk_urls.txt

          echo "Chunk ${{ matrix.chunk }}: URLs ${START}-${END} of ${TOTAL}"
          echo "URLs in this chunk:"
          cat /tmp/chunk_urls.txt

          # Skip if chunk is empty (can happen with uneven splits)
          if [ ! -s /tmp/chunk_urls.txt ]; then
            echo "No URLs in this chunk, skipping"
            echo "SKIP_CHUNK=true" >> $GITHUB_ENV
          fi

      - name: Run crawler (headless)
        if: env.SKIP_CHUNK != 'true'
        env:
          LLM_MAX_HTML_CHARS: 500000
        run: |
          python -m src.crawler.multi_capture \
            --urls /tmp/chunk_urls.txt \
            --headless \
            --with-llm \
            --loadmore-max 15

      - name: Upload crawl artifacts (optional)
        if: always()  # Upload even if crawler fails
        uses: actions/upload-artifact@v4
        with:
          name: crawl-results-${{ github.run_number }}-chunk-${{ matrix.chunk }}
          path: |
            out/*/manifest.json
            out/*/llm/*.jobs.json
          retention-days: 7
          if-no-files-found: ignore  # Don't fail if no files found

      - name: Report success
        if: success()
        run: |
          echo "✅ Crawl completed successfully"
          echo "Check Supabase for new job listings"

      - name: Report failure
        if: failure()
        run: |
          echo "❌ Crawl failed - check logs above"
          exit 1
